"""
Agent THERESE - Moteur de raisonnement multi-provider.

G√®re la boucle de conversation, le function calling,
le mode ultrathink, la m√©moire projet, et les commandes slash.
Support des images via Mistral Vision (Pixtral).

Providers support√©s:
- Mistral API (cloud) - d√©faut
- Ollama (local)
"""

import base64
import json
import mimetypes
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, AsyncIterator

from .config import ThereseConfig, config
from .memory import get_memory_manager
from .providers import ProviderBase, StreamChunk, get_provider
from .tools import TOOLS, get_tools_schema, get_tools_summary
from .tools.project import detect_project
from .checkpoints import CheckpointManager


@dataclass
class Message:
    """Message dans la conversation."""

    role: str  # "user", "assistant", "system", "tool"
    content: str
    tool_calls: list[dict] | None = None
    tool_call_id: str | None = None
    name: str | None = None
    images: list[str] | None = None  # Liste de chemins d'images


# Extensions d'images support√©es par Mistral Vision
IMAGE_EXTENSIONS = {".png", ".jpg", ".jpeg", ".gif", ".webp", ".bmp"}


def encode_image_to_base64(image_path: str) -> tuple[str, str]:
    """
    Encode une image en base64 pour l'API Mistral Vision.

    Returns:
        Tuple (base64_data, mime_type)
    """
    path = Path(image_path).expanduser().resolve()
    if not path.exists():
        raise FileNotFoundError(f"Image non trouv√©e: {image_path}")

    # D√©tecter le type MIME
    mime_type, _ = mimetypes.guess_type(str(path))
    if not mime_type:
        mime_type = "image/png"  # Fallback

    # Lire et encoder en base64
    with open(path, "rb") as f:
        data = base64.standard_b64encode(f.read()).decode("utf-8")

    return data, mime_type


def is_image_path(path: str) -> bool:
    """V√©rifie si un chemin pointe vers une image."""
    try:
        p = Path(path).expanduser()
        return p.suffix.lower() in IMAGE_EXTENSIONS
    except Exception:
        return False


@dataclass
class TokenUsage:
    """Suivi de l'utilisation des tokens."""

    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0

    def add(self, prompt: int, completion: int) -> None:
        self.prompt_tokens += prompt
        self.completion_tokens += completion
        self.total_tokens += prompt + completion

    def estimate_cost(self, model: str = "devstral-2") -> float:
        """Estime le co√ªt en USD."""
        # Prix Mistral (d√©cembre 2025)
        prices = {
            # Devstral 2 (code agents) - d√©c 2025
            "devstral-2": (0.0004, 0.002),  # $0.40/$2.00 per M tokens
            "devstral-small-2": (0.0001, 0.0003),  # $0.10/$0.30 per M tokens
            # Chat models
            "mistral-large-latest": (0.002, 0.006),  # input, output per 1K tokens
            "mistral-large-3-25-12": (0.002, 0.006),
            "mistral-small-latest": (0.0002, 0.0006),
            # Code models (legacy)
            "codestral-latest": (0.001, 0.003),
        }
        input_price, output_price = prices.get(model, (0.0004, 0.002))
        return (self.prompt_tokens * input_price + self.completion_tokens * output_price) / 1000


@dataclass
class ThereseAgent:
    """Agent principal THERESE."""

    config: ThereseConfig = field(default_factory=lambda: config)
    messages: list[Message] = field(default_factory=list)
    provider: ProviderBase | None = None
    usage: TokenUsage = field(default_factory=TokenUsage)
    checkpoint_manager: CheckpointManager | None = None

    def __post_init__(self) -> None:
        """Initialise le provider et le checkpoint manager."""
        self.config.validate()
        self._init_provider()
        self._init_checkpoint_manager()
        self._add_system_prompt()

    def _init_checkpoint_manager(self) -> None:
        """Initialise le gestionnaire de checkpoints."""
        try:
            self.checkpoint_manager = CheckpointManager(self.config.working_dir)
        except Exception:
            self.checkpoint_manager = None

    def _init_provider(self) -> None:
        """Initialise le provider LLM selon la config."""
        if self.config.provider == "ollama":
            self.provider = get_provider(
                "ollama",
                base_url=self.config.ollama_base_url,
            )
        else:
            self.provider = get_provider(
                "mistral",
                api_key=self.config.api_key,
            )

    def _get_project_context(self) -> str:
        """R√©cup√®re le contexte du projet."""
        try:
            info = detect_project(self.config.working_dir)
            memory = get_memory_manager(self.config.working_dir)

            context = f"""
## Projet actuel
- **Nom:** {info.name}
- **Type:** {info.type}
- **Langage:** {info.language}
- **Package Manager:** {info.package_manager or 'N/A'}
"""
            if info.frameworks:
                context += f"- **Frameworks:** {', '.join(info.frameworks)}\n"

            if info.scripts:
                context += "\n**Scripts disponibles:** " + ", ".join(list(info.scripts.keys())[:5])

            # Ajouter la m√©moire si elle existe
            memory_context = memory.get_context()
            if memory_context:
                context += "\n" + memory_context

            return context
        except Exception:
            return ""

    def _add_system_prompt(self) -> None:
        """Ajoute le prompt syst√®me."""
        tools_summary = get_tools_summary()
        project_context = self._get_project_context()

        system_prompt = f"""Tu es TH√âR√àSE, un assistant de programmation expert propuls√© par Mistral 3.

‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üá´üá∑ Tu es fran√ßaise, tu parles fran√ßais, tu codes comme une chef.

## Ton r√¥le
Tu aides les d√©veloppeurs √† :
- Explorer et comprendre leur codebase
- Lire, √©crire et modifier des fichiers
- Ex√©cuter des commandes shell
- G√©rer Git (commits, branches, status)
- Rechercher sur le web
- D√©tecter et configurer des projets
- Suivre les t√¢ches en cours

## R√©pertoire de travail
`{self.config.working_dir}`

{project_context}

## Tes outils ({len(TOOLS)} disponibles)
{tools_summary}

## Commandes slash
L'utilisateur peut utiliser des commandes commen√ßant par `/`:
- `/help` : Affiche l'aide
- `/init` : Initialise THERESE pour le projet
- `/status` : Statut Git
- `/tree` : Arborescence
- `/tasks` : T√¢ches en cours
- `/memory` : M√©moire projet
- `/model` : Changer de mod√®le
- `/mode` : Mode d'approbation (auto/safe/yolo)

## Mode d'approbation actuel: `{self.config.mode}`
- `auto`: Confirmation pour les actions dangereuses
- `safe`: Confirmation pour toutes les modifications
- `yolo`: Aucune confirmation

## R√®gles d'or
1. **Lis TOUJOURS** un fichier avant de le modifier
2. Utilise `diff_preview` avant les modifications importantes
3. Utilise `task_add` pour planifier les t√¢ches complexes
4. Sois concis et direct dans tes r√©ponses
5. En cas d'erreur, analyse et r√©essaie
6. Parle fran√ßais, code proprement
7. Utilise `git_status` avant les commits

## Style
- R√©ponses courtes et directes
- Markdown pour le formatage
- Citations de code avec num√©ros de ligne
- Pas d'emojis sauf si demand√©

Allez, au boulot !"""

        self.messages.append(Message(role="system", content=system_prompt))

    async def _execute_tool(self, name: str, arguments: dict[str, Any]) -> str:
        """Ex√©cute un outil et retourne le r√©sultat (async)."""
        tool = TOOLS.get(name)
        if not tool:
            return f"Erreur: outil '{name}' non trouv√©"

        try:
            result = await tool.execute(**arguments)

            # Tracker les changements dans la m√©moire
            if name in ("write_file", "edit_file") and result.success:
                memory = get_memory_manager(self.config.working_dir)
                file_path = arguments.get("file_path", "unknown")
                memory.add_change(f"Modifi√©: {file_path}")

            return result.to_string()
        except Exception as e:
            return f"Erreur d'ex√©cution de {name}: {e}"

    def _execute_tool_sync(self, name: str, arguments: dict[str, Any]) -> str:
        """Ex√©cute un outil de mani√®re synchrone (pour chat_sync)."""
        import asyncio

        tool = TOOLS.get(name)
        if not tool:
            return f"Erreur: outil '{name}' non trouv√©"

        try:
            # Auto-checkpoint AVANT les modifications de fichiers
            if name in ("write_file", "edit_file") and self.checkpoint_manager:
                file_path = arguments.get("file_path", "unknown")
                try:
                    self.checkpoint_manager.track_file(Path(file_path))
                    self.checkpoint_manager.auto_checkpoint(before_action=f"{name} {file_path}")
                except Exception:
                    pass  # Ne pas bloquer si checkpoint √©choue

            # Nettoyer toute loop existante avant d'en cr√©er une nouvelle
            try:
                asyncio.set_event_loop(None)
            except Exception:
                pass

            # Cr√©er une nouvelle event loop isol√©e
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                result = loop.run_until_complete(tool.execute(**arguments))
            finally:
                loop.close()
                asyncio.set_event_loop(None)

            # Tracker les changements dans la m√©moire
            if name in ("write_file", "edit_file") and result.success:
                memory = get_memory_manager(self.config.working_dir)
                file_path = arguments.get("file_path", "unknown")
                memory.add_change(f"Modifi√©: {file_path}")

            return result.to_string()
        except Exception as e:
            return f"Erreur d'ex√©cution de {name}: {e}"

    def _get_ollama_tools(self) -> list[dict]:
        """Retourne un subset de tools essentiels pour Ollama.

        21 tools = trop de contexte pour les mod√®les locaux.
        On garde les 8 outils les plus importants pour le coding.
        """
        essential_tools = [
            "read_file",    # Lire du code
            "write_file",   # √âcrire du code
            "edit_file",    # Modifier du code
            "bash",         # Ex√©cuter des commandes
            "tree",         # Explorer le projet
            "grep",         # Rechercher dans le code
            "glob",         # Trouver des fichiers
            "git_status",   # Voir l'√©tat Git
        ]

        all_tools = get_tools_schema()
        return [t for t in all_tools if t["function"]["name"] in essential_tools]

    def _messages_to_provider_format(self, images: list[str] | None = None) -> list[dict]:
        """Convertit les messages au format g√©n√©rique pour les providers."""
        result = []
        for msg in self.messages:
            msg_dict = {
                "role": msg.role,
                "content": msg.content,
            }
            if msg.images:
                # Encoder les images en base64 pour le provider
                encoded_images = []
                for img_path in msg.images:
                    try:
                        b64_data, mime_type = encode_image_to_base64(img_path)
                        encoded_images.append({
                            "url": f"data:{mime_type};base64,{b64_data}",
                            "base64": b64_data,
                        })
                    except Exception:
                        pass
                if encoded_images:
                    msg_dict["images"] = encoded_images
            if msg.tool_calls:
                msg_dict["tool_calls"] = msg.tool_calls
            if msg.tool_call_id:
                msg_dict["tool_call_id"] = msg.tool_call_id
            if msg.name:
                msg_dict["name"] = msg.name
            result.append(msg_dict)
        return result

    def chat_sync(
        self, user_input: str, images: list[str] | None = None
    ):
        """
        Version SYNCHRONE de chat() pour √©viter les probl√®mes d'event loop.

        Utilise le provider configur√© (Mistral API ou Ollama).
        Doit √™tre appel√©e depuis un thread s√©par√©.

        Yields des chunks de texte pour l'affichage streaming.
        """
        import asyncio

        # IMPORTANT: Nettoyer toute r√©f√©rence √† une event loop existante
        try:
            asyncio.set_event_loop(None)
        except Exception:
            pass

        # Recr√©er le provider (thread safety)
        self._init_provider()

        # FIX: V√©rifier si le dernier message est un "tool"
        if self.messages and self.messages[-1].role == "tool":
            self.messages.append(Message(
                role="assistant",
                content="(Reprise de la conversation apr√®s interruption)"
            ))

        # Ajouter le message utilisateur (avec images si pr√©sentes)
        self.messages.append(Message(role="user", content=user_input, images=images))

        # D√©terminer le mod√®le selon le provider
        if self.config.provider == "ollama":
            model = self.config.ollama_model
        else:
            # Utiliser Pixtral pour les images (Mistral), sinon le mod√®le par d√©faut
            model = "pixtral-large-latest" if images else self.config.model

        max_iterations = 15

        for iteration in range(max_iterations):
            # Pr√©parer les messages pour le provider
            provider_messages = self._messages_to_provider_format(images)

            # Pr√©parer les tools (sauf premi√®re it√©ration avec images sur Mistral)
            tools = None
            if not (images and iteration == 0 and self.config.provider == "mistral"):
                if self.provider and self.provider.supports_tools:
                    # Ollama : subset de tools essentiels (21 tools = trop pour le contexte)
                    if self.config.provider == "ollama":
                        tools = self._get_ollama_tools()
                    else:
                        tools = get_tools_schema()

            # Appel streaming via le provider
            content_chunks: list[str] = []
            tool_calls: list[dict] = []

            try:
                for chunk in self.provider.chat_stream(
                    messages=provider_messages,
                    model=model if iteration == 0 else self.config.get_active_model(),
                    tools=tools,
                ):
                    # Contenu textuel
                    if chunk.content:
                        content_chunks.append(chunk.content)
                        yield chunk.content

                    # Tool calls
                    if chunk.tool_calls:
                        tool_calls = chunk.tool_calls

                    # Usage
                    if chunk.usage:
                        self.usage.add(
                            chunk.usage.get("prompt_tokens", 0),
                            chunk.usage.get("completion_tokens", 0),
                        )

            except Exception as e:
                yield f"\n\n‚ùå Erreur provider: {e}"
                return

            full_content = "".join(content_chunks)

            # Si pas de tool calls, on a fini
            if not tool_calls:
                self.messages.append(Message(
                    role="assistant",
                    content=full_content,
                ))
                break

            # Ajouter le message assistant avec tool calls
            self.messages.append(Message(
                role="assistant",
                content=full_content,
                tool_calls=tool_calls,
            ))

            # Ex√©cuter les outils (de mani√®re synchrone)
            for tc in tool_calls:
                func_name = tc["function"]["name"]
                try:
                    func_args = json.loads(tc["function"]["arguments"])
                except json.JSONDecodeError:
                    func_args = {}

                yield f"\n\n‚öôÔ∏è  **{func_name}**"
                if func_args:
                    args_preview = []
                    for k, v in func_args.items():
                        v_str = repr(v)
                        if len(v_str) > 40:
                            v_str = v_str[:40] + "..."
                        args_preview.append(f"{k}={v_str}")
                    yield f"({', '.join(args_preview)})\n"
                else:
                    yield "()\n"

                # Ex√©cuter l'outil de mani√®re synchrone
                result = self._execute_tool_sync(func_name, func_args)

                lines = result.split("\n")
                if len(lines) > 30:
                    result_preview = "\n".join(lines[:30]) + f"\n... ({len(lines) - 30} lignes de plus)"
                elif len(result) > 2000:
                    result_preview = result[:2000] + "..."
                else:
                    result_preview = result

                yield f"\n{result_preview}\n"

                self.messages.append(Message(
                    role="tool",
                    content=result,
                    tool_call_id=tc["id"],
                    name=func_name,
                ))

        else:
            yield "\n\n‚ö†Ô∏è Limite d'it√©rations atteinte. La t√¢che est peut-√™tre trop complexe."

        # Auto-compact si n√©cessaire (apr√®s la r√©ponse)
        compacted, compact_msg = self.auto_compact()
        if compacted:
            yield f"\n\n{compact_msg}"

    def reset(self) -> None:
        """R√©initialise la conversation."""
        self.messages.clear()
        self._add_system_prompt()

    def _should_auto_compact(self) -> bool:
        """V√©rifie si on doit auto-compacter bas√© sur les tokens."""
        if not self.config.auto_compact:
            return False
        threshold = int(self.config.max_context_tokens * self.config.compact_threshold)
        return self.usage.prompt_tokens > threshold

    def _format_messages_for_summary(self, messages: list[Message]) -> str:
        """Formate les messages pour le r√©sum√©."""
        formatted = []
        for msg in messages:
            if msg.role == "system":
                continue
            prefix = "üë§" if msg.role == "user" else "ü§ñ" if msg.role == "assistant" else "üîß"
            content = msg.content[:500] if msg.content else ""
            if msg.tool_calls:
                tools = [tc["function"]["name"] for tc in msg.tool_calls]
                content += f" [Tools: {', '.join(tools)}]"
            formatted.append(f"{prefix} {content}")
        return "\n\n".join(formatted)

    def _generate_summary_sync(self, messages: list[Message]) -> str:
        """G√©n√®re un r√©sum√© LLM des messages (sync)."""
        formatted = self._format_messages_for_summary(messages)
        summary_prompt = f"""R√©sume cette conversation en 3-5 points cl√©s.
Garde les informations importantes : fichiers modifi√©s, d√©cisions prises, probl√®mes r√©solus.
Sois concis (max 300 mots).

Conversation:
{formatted[:8000]}

R√©sum√©:"""

        try:
            # Utiliser un mod√®le rapide selon le provider
            if self.config.provider == "ollama":
                model = "ministral-3:3b"  # Rapide pour r√©sum√©
            else:
                model = "mistral-small-latest"

            # R√©cup√©rer le r√©sum√© via le provider
            summary_parts = []
            for chunk in self.provider.chat_stream(
                messages=[{"role": "user", "content": summary_prompt}],
                model=model,
            ):
                if chunk.content:
                    summary_parts.append(chunk.content)

            return "".join(summary_parts) or "[R√©sum√© indisponible]"
        except Exception as e:
            return f"[R√©sum√© auto: {len(messages)} messages pr√©c√©dents - Erreur: {e}]"

    def auto_compact(self) -> tuple[bool, str]:
        """
        Auto-compacte si n√©cessaire. Utilise le LLM pour r√©sumer.

        Returns:
            (compacted: bool, message: str)
        """
        if not self._should_auto_compact():
            return False, ""

        if len(self.messages) <= self.config.compact_keep_recent + 2:
            return False, ""

        # S√©parer les messages
        system_msg = self.messages[0]
        recent = self.messages[-self.config.compact_keep_recent:]
        old_messages = self.messages[1:-self.config.compact_keep_recent]

        if not old_messages:
            return False, ""

        # S'assurer que recent commence par un message "user" pour un ordre valide
        # Sinon on risque: assistant (r√©sum√©) -> tool -> user (invalide)
        while recent and recent[0].role in ("tool", "assistant"):
            recent = recent[1:]

        if not recent:
            return False, ""

        # G√©n√©rer un r√©sum√© intelligent
        summary = self._generate_summary_sync(old_messages)

        # Reconstruire
        old_count = len(self.messages)
        self.messages = [system_msg]
        self.messages.append(Message(
            role="assistant",
            content=f"üìù **R√©sum√© de la conversation pr√©c√©dente:**\n\n{summary}"
        ))
        self.messages.extend(recent)

        # Reset partiel des tokens (estimation)
        self.usage.prompt_tokens = int(self.usage.prompt_tokens * 0.3)

        return True, f"üíæ Conversation compact√©e: {old_count} ‚Üí {len(self.messages)} messages"

    def compact(self) -> str:
        """Compacte manuellement la conversation avec r√©sum√© LLM."""
        if len(self.messages) <= 2:
            return "Conversation trop courte pour √™tre compact√©e."

        compacted, message = self.auto_compact()
        if compacted:
            return message
        return "Rien √† compacter."

    def get_stats(self) -> dict:
        """Retourne les statistiques de la session."""
        return {
            "messages": len(self.messages),
            "tokens": {
                "prompt": self.usage.prompt_tokens,
                "completion": self.usage.completion_tokens,
                "total": self.usage.total_tokens,
            },
            "cost_usd": round(self.usage.estimate_cost(self.config.model), 4),
            "model": self.config.model,
            "mode": self.config.mode,
        }
