"""
Agent THERESE - Moteur de raisonnement avec Mistral 3.

G√®re la boucle de conversation, le function calling,
le mode ultrathink, la m√©moire projet, et les commandes slash.
"""

import json
from dataclasses import dataclass, field
from typing import Any, AsyncIterator

from mistralai import Mistral
from mistralai.models import (
    AssistantMessage,
    SystemMessage,
    ToolMessage,
    UserMessage,
)

from .config import ThereseConfig, config
from .memory import get_memory_manager
from .tools import TOOLS, get_tools_schema, get_tools_summary
from .tools.project import detect_project


@dataclass
class Message:
    """Message dans la conversation."""

    role: str  # "user", "assistant", "system", "tool"
    content: str
    tool_calls: list[dict] | None = None
    tool_call_id: str | None = None
    name: str | None = None


@dataclass
class TokenUsage:
    """Suivi de l'utilisation des tokens."""

    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0

    def add(self, prompt: int, completion: int) -> None:
        self.prompt_tokens += prompt
        self.completion_tokens += completion
        self.total_tokens += prompt + completion

    def estimate_cost(self, model: str = "mistral-large-latest") -> float:
        """Estime le co√ªt en USD."""
        # Prix approximatifs Mistral (d√©cembre 2024)
        prices = {
            "mistral-large-latest": (0.002, 0.006),  # input, output per 1K tokens
            "mistral-large-3-25-12": (0.002, 0.006),
            "codestral-latest": (0.001, 0.003),
            "mistral-small-latest": (0.0002, 0.0006),
        }
        input_price, output_price = prices.get(model, (0.002, 0.006))
        return (self.prompt_tokens * input_price + self.completion_tokens * output_price) / 1000


@dataclass
class ThereseAgent:
    """Agent principal THERESE."""

    config: ThereseConfig = field(default_factory=lambda: config)
    messages: list[Message] = field(default_factory=list)
    client: Mistral | None = None
    usage: TokenUsage = field(default_factory=TokenUsage)

    def __post_init__(self) -> None:
        """Initialise le client Mistral."""
        self.config.validate()
        self.client = Mistral(api_key=self.config.api_key)
        self._add_system_prompt()

    def _get_project_context(self) -> str:
        """R√©cup√®re le contexte du projet."""
        try:
            info = detect_project(self.config.working_dir)
            memory = get_memory_manager(self.config.working_dir)

            context = f"""
## Projet actuel
- **Nom:** {info.name}
- **Type:** {info.type}
- **Langage:** {info.language}
- **Package Manager:** {info.package_manager or 'N/A'}
"""
            if info.frameworks:
                context += f"- **Frameworks:** {', '.join(info.frameworks)}\n"

            if info.scripts:
                context += "\n**Scripts disponibles:** " + ", ".join(list(info.scripts.keys())[:5])

            # Ajouter la m√©moire si elle existe
            memory_context = memory.get_context()
            if memory_context:
                context += "\n" + memory_context

            return context
        except Exception:
            return ""

    def _add_system_prompt(self) -> None:
        """Ajoute le prompt syst√®me."""
        tools_summary = get_tools_summary()
        project_context = self._get_project_context()

        system_prompt = f"""Tu es TH√âR√àSE, un assistant de programmation expert propuls√© par Mistral 3.

‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù
   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù
   ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó
   ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üá´üá∑ Tu es fran√ßaise, tu parles fran√ßais, tu codes comme une chef.

## Ton r√¥le
Tu aides les d√©veloppeurs √† :
- Explorer et comprendre leur codebase
- Lire, √©crire et modifier des fichiers
- Ex√©cuter des commandes shell
- G√©rer Git (commits, branches, status)
- Rechercher sur le web
- D√©tecter et configurer des projets
- Suivre les t√¢ches en cours

## R√©pertoire de travail
`{self.config.working_dir}`

{project_context}

## Tes outils ({len(TOOLS)} disponibles)
{tools_summary}

## Commandes slash
L'utilisateur peut utiliser des commandes commen√ßant par `/`:
- `/help` : Affiche l'aide
- `/init` : Initialise THERESE pour le projet
- `/status` : Statut Git
- `/tree` : Arborescence
- `/tasks` : T√¢ches en cours
- `/memory` : M√©moire projet
- `/model` : Changer de mod√®le
- `/mode` : Mode d'approbation (auto/safe/yolo)

## Mode d'approbation actuel: `{self.config.mode}`
- `auto`: Confirmation pour les actions dangereuses
- `safe`: Confirmation pour toutes les modifications
- `yolo`: Aucune confirmation

## R√®gles d'or
1. **Lis TOUJOURS** un fichier avant de le modifier
2. Utilise `diff_preview` avant les modifications importantes
3. Utilise `task_add` pour planifier les t√¢ches complexes
4. Sois concis et direct dans tes r√©ponses
5. En cas d'erreur, analyse et r√©essaie
6. Parle fran√ßais, code proprement
7. Utilise `git_status` avant les commits

## Style
- R√©ponses courtes et directes
- Markdown pour le formatage
- Citations de code avec num√©ros de ligne
- Pas d'emojis sauf si demand√©

Allez, au boulot !"""

        self.messages.append(Message(role="system", content=system_prompt))

    def _messages_to_mistral(self) -> list:
        """Convertit les messages au format Mistral."""
        result = []
        for msg in self.messages:
            if msg.role == "system":
                result.append(SystemMessage(content=msg.content))
            elif msg.role == "user":
                result.append(UserMessage(content=msg.content))
            elif msg.role == "assistant":
                if msg.tool_calls:
                    result.append(AssistantMessage(
                        content=msg.content or "",
                        tool_calls=msg.tool_calls,
                    ))
                else:
                    result.append(AssistantMessage(content=msg.content))
            elif msg.role == "tool":
                result.append(ToolMessage(
                    content=msg.content,
                    tool_call_id=msg.tool_call_id,
                    name=msg.name,
                ))
        return result

    async def _execute_tool(self, name: str, arguments: dict[str, Any]) -> str:
        """Ex√©cute un outil et retourne le r√©sultat."""
        tool = TOOLS.get(name)
        if not tool:
            return f"Erreur: outil '{name}' non trouv√©"

        try:
            result = await tool.execute(**arguments)

            # Tracker les changements dans la m√©moire
            if name in ("write_file", "edit_file") and result.success:
                memory = get_memory_manager(self.config.working_dir)
                file_path = arguments.get("file_path", "unknown")
                memory.add_change(f"Modifi√©: {file_path}")

            return result.to_string()
        except Exception as e:
            return f"Erreur d'ex√©cution de {name}: {e}"

    async def chat(self, user_input: str) -> AsyncIterator[str]:
        """
        Envoie un message et stream la r√©ponse.

        Yields des chunks de texte pour l'affichage streaming.
        G√®re automatiquement les tool calls.
        """
        # Ajouter le message utilisateur
        self.messages.append(Message(role="user", content=user_input))

        max_iterations = 15  # Augment√© pour les t√¢ches complexes

        for iteration in range(max_iterations):
            # Appel √† Mistral avec streaming
            response = self.client.chat.stream(
                model=self.config.model,
                messages=self._messages_to_mistral(),
                tools=get_tools_schema(),
                tool_choice="auto",
            )

            # Collecter la r√©ponse
            content_chunks: list[str] = []
            tool_calls: list[dict] = []

            for event in response:
                if event.data.choices:
                    choice = event.data.choices[0]
                    delta = choice.delta

                    # Contenu textuel
                    if delta.content:
                        content_chunks.append(delta.content)
                        yield delta.content

                    # Tool calls
                    if delta.tool_calls:
                        for tc in delta.tool_calls:
                            # G√©rer l'accumulation des tool calls en streaming
                            if tc.index >= len(tool_calls):
                                tool_calls.append({
                                    "id": tc.id or "",
                                    "type": "function",
                                    "function": {
                                        "name": tc.function.name or "",
                                        "arguments": tc.function.arguments or "",
                                    },
                                })
                            else:
                                # Accumuler les arguments
                                if tc.function.arguments:
                                    tool_calls[tc.index]["function"]["arguments"] += tc.function.arguments
                                if tc.function.name:
                                    tool_calls[tc.index]["function"]["name"] = tc.function.name
                                if tc.id:
                                    tool_calls[tc.index]["id"] = tc.id

                # Tracker l'usage
                if event.data.usage:
                    self.usage.add(
                        event.data.usage.prompt_tokens or 0,
                        event.data.usage.completion_tokens or 0,
                    )

            full_content = "".join(content_chunks)

            # Si pas de tool calls, on a fini
            if not tool_calls:
                self.messages.append(Message(
                    role="assistant",
                    content=full_content,
                ))
                break

            # Ajouter le message assistant avec tool calls
            self.messages.append(Message(
                role="assistant",
                content=full_content,
                tool_calls=tool_calls,
            ))

            # Ex√©cuter les outils
            for tc in tool_calls:
                func_name = tc["function"]["name"]
                try:
                    func_args = json.loads(tc["function"]["arguments"])
                except json.JSONDecodeError:
                    func_args = {}

                # Afficher l'appel d'outil
                yield f"\n\n‚öôÔ∏è  **{func_name}**"
                if func_args:
                    # Tronquer les arguments longs
                    args_preview = []
                    for k, v in func_args.items():
                        v_str = repr(v)
                        if len(v_str) > 40:
                            v_str = v_str[:40] + "..."
                        args_preview.append(f"{k}={v_str}")
                    yield f"({', '.join(args_preview)})\n"
                else:
                    yield "()\n"

                # Ex√©cuter l'outil
                result = await self._execute_tool(func_name, func_args)

                # Afficher un r√©sum√© du r√©sultat
                lines = result.split("\n")
                if len(lines) > 10:
                    result_preview = "\n".join(lines[:10]) + f"\n... ({len(lines) - 10} lignes de plus)"
                elif len(result) > 500:
                    result_preview = result[:500] + "..."
                else:
                    result_preview = result

                yield f"```\n{result_preview}\n```\n"

                # Ajouter le r√©sultat aux messages
                self.messages.append(Message(
                    role="tool",
                    content=result,
                    tool_call_id=tc["id"],
                    name=func_name,
                ))

        else:
            yield "\n\n‚ö†Ô∏è Limite d'it√©rations atteinte. La t√¢che est peut-√™tre trop complexe."

    def reset(self) -> None:
        """R√©initialise la conversation."""
        self.messages.clear()
        self._add_system_prompt()

    def compact(self) -> str:
        """Compacte la conversation en gardant un r√©sum√©."""
        if len(self.messages) <= 2:
            return "Conversation trop courte pour √™tre compact√©e."

        # Garder le syst√®me et les 5 derniers √©changes
        system_msg = self.messages[0]
        recent = self.messages[-10:]

        # Cr√©er un r√©sum√©
        summary = f"[Conversation pr√©c√©dente r√©sum√©e - {len(self.messages)} messages]"

        self.messages = [system_msg]
        self.messages.append(Message(role="assistant", content=summary))
        self.messages.extend(recent)

        return f"Conversation compact√©e: {len(self.messages)} messages conserv√©s."

    def get_stats(self) -> dict:
        """Retourne les statistiques de la session."""
        return {
            "messages": len(self.messages),
            "tokens": {
                "prompt": self.usage.prompt_tokens,
                "completion": self.usage.completion_tokens,
                "total": self.usage.total_tokens,
            },
            "cost_usd": round(self.usage.estimate_cost(self.config.model), 4),
            "model": self.config.model,
            "mode": self.config.mode,
        }
